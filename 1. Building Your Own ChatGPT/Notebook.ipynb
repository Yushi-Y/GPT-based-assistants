{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Building AI Assistants Part I: Build Your Own ChatGPT\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/life-efficient/A23/blob/main/1.%20Building%20AI%20Assistants%20Part%20I%3A%20Your%20Own%20ChatGPT/Solutions.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "[Find the solutions here](https://colab.research.google.com/github/life-efficient/A23/blob/main/1.%20Building%20AI%20Assistants%20Part%20I%3A%20Your%20Own%20ChatGPT/Solutions.ipynb)\n",
        "\n",
        "[Find the slides here](https://tome.app/build-the-future-5f1/building-stuff-with-generative-ai-lecture-1-clnwck0490021mv7bmca3kosi)\n",
        "\n",
        "[Access Discord](https://discord.gg/SBW2zmfSMh)\n",
        "\n",
        "![](images/cyber.png)\n",
        "\n",
        "## Motivation\n",
        "\n",
        "Have you ever watched a sci-fi movie and thought how amazing it would be to have your own friendly AI assistant, like Jarvis in Iron Man? Well, the future is now, because we have all the tools to build these kinds of systems, and that's exactly what we're going to do in this notebook.\n",
        "\n",
        "In just this notebook, you're going to unlock the power to use the OpenAI API, learn insider tips for prompt engineering, and understand how the AI engineering behind ChatGPT works.\n",
        "\n",
        "Let's get started and build our own AI assistant!\n",
        "\n",
        "## A Simple Start\n",
        "\n",
        "Let's start the conversation by:\n",
        "1. Defining a start message that our assistant will read out.\n",
        "2. Allowing the user to input a response - this is the beginning of defining a loop of back-and-forth conversation.\n",
        "\n",
        "> Try to look up how to do this yourself. If you get stuck, [here](https://www.google.com/search?q=python+get+user+input) are the search results you should look at."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What can I do to help?\n"
          ]
        }
      ],
      "source": [
        "message = \"What can I do to help?\" # defining initial message from assistant\n",
        "print(\"Assistant:\", message)\n",
        "user_input = input(\"Enter a prompt...\") # TODO take user input\n",
        "print(\"User:\", user_input)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we need to somehow make a request to an AI system that can interpret the prompt and come up with a response.\n",
        "\n",
        "To use powerful AI systems like GPT4 to provide responses to our messages, we can use the `openai` library (code they have written).\n",
        "\n",
        "Questions:\n",
        "- What's an AI model?\n",
        "- What is GPT?\n",
        "- What's GPT3 vs GPT3.5 vs GPT4?\n",
        "\n",
        "We firstly download that Python library from the internet.\n",
        "\n",
        "> Note: code cells starting with `!` run [bash](https://www.gnu.org/software/bash/) (a language used to talk directly to the operating system), rather than running Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (0.28.1)\n",
            "Requirement already satisfied: tqdm in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from openai) (4.64.1)\n",
            "Requirement already satisfied: requests>=2.20 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: aiohttp in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from openai) (3.8.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from requests>=2.20->openai) (2.10)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from requests>=2.20->openai) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from requests>=2.20->openai) (1.26.6)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from aiohttp->openai) (5.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from aiohttp->openai) (1.6.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from aiohttp->openai) (21.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from aiohttp->openai) (1.2.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from aiohttp->openai) (4.0.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from aiohttp->openai) (1.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.5 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from async-timeout<5.0,>=4.0.0a3->aiohttp->openai) (4.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we need to import the library into our Python code in the next cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "xNVTrmzFk6pE",
        "outputId": "e4b40193-c507-4642-ee5a-148f0f9283ea"
      },
      "outputs": [],
      "source": [
        "import openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This Python library contains a bunch of tools that we can use to interact with the OpenAI API.\n",
        "\n",
        "But firstly, let's make sure we're all confident answering the question: What is an API?\n",
        "\n",
        "An API is simply a service running on a computer that understands how to process requests from users and provide relevant responses.\n",
        "\n",
        "For example\n",
        "- The Uber API:\n",
        "    - Request: Get me a ride!\n",
        "    - Response: Ride details\n",
        "    - Many other things\n",
        "- The OpenAI API:\n",
        "    - Request: Your prompt\n",
        "    - Response: GPT4's reply\n",
        "\n",
        "\n",
        "![](images/API.png)\n",
        "\n",
        "Nowadays every big company has an API (some are publicly accessible, others are used internally).\n",
        "\n",
        "\n",
        "Questions\n",
        "- Can anyone name any other APIs they know exist?\n",
        "- What requests can they take and what do they respond with?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRkIhx8TlWCU"
      },
      "source": [
        "\n",
        "Because OpenAI needs to track which, and how users are using the API, we need to provide a \"token\" which is essentially like a password.\n",
        "\n",
        "You can find your OpenAI API key [here](https://platform.openai.com/account/api-keys).\n",
        "\n",
        "Note: You will need to ensure you've completed the following steps for our later requests to OpenAI to work.\n",
        "\n",
        "1. Create an OpenAI account\n",
        "2. Set up a payment method\n",
        "\n",
        "Here is the simple setup for using the OpenAI API:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "iLzSohd0le5b",
        "outputId": "eaac69cf-351c-470e-801c-f4bde8a0e6b5"
      },
      "outputs": [],
      "source": [
        "# TODO # get the openai library's api_key attribute and set it equal to your api key\n",
        "openai.api_key = \"YOUR API KEY HERE\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have the API set up, let's make our first request.\n",
        "\n",
        "The cell below shows where that fits into our code so far, if we put all of the Python we've written into one cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What can I do to help?\n"
          ]
        }
      ],
      "source": [
        "import openai\n",
        "# openai.api_key = \"YOUR API KEY HERE\" # commented out so you don't overwrite the correct api key you set above\n",
        "\n",
        "message = \"What can I do to help?\"\n",
        "print(message)\n",
        "user_input = input(\"Type a prompt...\") # we will use this in the next cell and send it to GPT\n",
        "# now we need to process that user input to provide a response\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To make the request to GPT using the OpenAI API, we can read the [documentation](https://platform.openai.com/docs/api-reference) that describes how to do that.\n",
        "\n",
        "Making the request requires at least two things:\n",
        "1. The messages in the conversation so far (including our prompt)\n",
        "2. The name of the AI engine (the \"model\") that we want to ask for a response.\n",
        "\n",
        "Make sure to [look into](https://platform.openai.com/docs/models/model-endpoint-compatibility) the differences and trade-offs between each choice of model, which you'll need to define in the function call."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"id\": \"chatcmpl-8FTFA8zWd2IEAHwiAhn76Br1i7nk2\",\n",
            "  \"object\": \"chat.completion\",\n",
            "  \"created\": 1698698024,\n",
            "  \"model\": \"gpt-3.5-turbo-0613\",\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"index\": 0,\n",
            "      \"message\": {\n",
            "        \"role\": \"assistant\",\n",
            "        \"content\": \"Hello! How can I assist you today?\"\n",
            "      },\n",
            "      \"finish_reason\": \"stop\"\n",
            "    }\n",
            "  ],\n",
            "  \"usage\": {\n",
            "    \"prompt_tokens\": 8,\n",
            "    \"completion_tokens\": 9,\n",
            "    \"total_tokens\": 17\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# TODO create a list of messages that includes just one message in the format expected by the openai API\n",
        "messages =  [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": user_input\n",
        "    }\n",
        "]\n",
        "\n",
        "response = openai.ChatCompletion.create( # TODO use the openai library to create a chat completion\n",
        "    model=\"gpt-3.5-turbo\", # TODO pick the model you want to use\n",
        "    messages=messages, # TODO set this equal to the messages variable we created above\n",
        "    max_tokens=30 # TODO set the max tokens \n",
        ")\n",
        "\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Questions:\n",
        "- What model did you choose and why? What were some factors to consider?\n",
        "- What else came along with the response that you didn't expect?\n",
        "- What other parameters could you have provided with your request and what do they do?\n",
        "\n",
        "This response contains more than we need. Now we need to index the content out of it "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello! How can I assist you today?\n"
          ]
        }
      ],
      "source": [
        "content = response[\"choices\"][0][\"message\"][\"content\"] # TODO get the content from the response\n",
        "print(content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When you run the code above, you should see the AI system's response printed to the console. Congratulations! You have just made your first request to an AI system using the OpenAI API.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's put that code into a function, so it's all defined under one name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "aagcABjllrTo"
      },
      "outputs": [],
      "source": [
        "# TODO define a function called get_response that takes in a list of messages in the OpenAI format and returns the content of the response\n",
        "def get_response(messages):\n",
        "\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        max_tokens=30\n",
        "    )\n",
        "\n",
        "    content = response[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "    return content\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can call this function whenever we want, as shown below. We've encapsulated some much longer and more complicated looking code into a single, short line. This is going to make things super easy for us later!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'role': 'user', 'content': 'hi'}]\n",
            "Hello! How can I assist you today?\n"
          ]
        }
      ],
      "source": [
        "print(messages)\n",
        "response = get_response(messages)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m63_Apk0l1Cr"
      },
      "source": [
        "Now that we have defined the `request` function, let's test it out with a simple prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "hHcqdo2nmVo1",
        "outputId": "ece8155e-9e0b-4ad8-ad29-38544f078e53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: Tell me a story.\n",
            "Assistant: Once upon a time, in a small village nestled between the lush green mountains, lived a young girl named Lily. She was known for her kind heart\n"
          ]
        }
      ],
      "source": [
        "messages = []\n",
        "prompt = \"Tell me a story.\"\n",
        "print(\"User:\", prompt)\n",
        "message = {\"role\": \"user\", \"content\": prompt}\n",
        "messages.append(message)\n",
        "\n",
        "response = get_response(messages)\n",
        "print(\"Assistant:\", response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVp7M2TXmeII"
      },
      "source": [
        "# Coding the Chat Loop\n",
        "\n",
        "In the previous section, we learned how to make our first request to an AI system and receive a response. However, the conversation always ended after one response from the AI system. Now, let's make the conversation continuous by coding the chat loop.\n",
        "\n",
        "We will need to:\n",
        "1. Put the code we've written into a loop that runs continuously\n",
        "2. Add the assistant messages to our running list of messages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: hello\n",
            "Assistant: Hello! How may I assist you today?\n",
            "User: exit\n",
            "Assistant: Goodbye!\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/3z/29w5rr9d0k3_p863hm40sdnc0000gn/T/ipykernel_6115/3965266052.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmessages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Type a prompt...\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# TODO get user input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"User:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0muser_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;31m# TODO create user message dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1005\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m             )\n\u001b[0;32m-> 1007\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1008\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1050\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1052\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1053\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "messages = []\n",
        "while True:\n",
        "    prompt = input(\"Type a prompt...\") # TODO get user input\n",
        "    print(\"User:\", prompt)\n",
        "    user_message = {\"role\": \"user\", \"content\": prompt} # TODO create user message dictionary\n",
        "    messages.append(user_message) # TODO add message to list of messages\n",
        "    response = get_response(messages) # \n",
        "    assistant_message = {\"role\": \"assistant\", \"content\": prompt}\n",
        "    messages.append(assistant_message)\n",
        "    print(\"Assistant:\", response)\n",
        "    # break # REMOVE ME TO RUN THE CHAT LOOP\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> NOTE: YOU WILL NEED TO INTERRUPT THE NOTEBOOK TO STOP THIS LOOP (there should be a button at the top).\n",
        "\n",
        "To make this simpler, let's add an option for the user to exit if the prompt they type is exactly \"exit\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: exit\n",
            "Exiting chat\n"
          ]
        }
      ],
      "source": [
        "messages = []\n",
        "while True:\n",
        "    prompt = input(\"Type a prompt...\")\n",
        "    print(\"User:\", prompt)\n",
        "    if prompt == \"exit\": # TODO if the user types exit\n",
        "        print(\"Exiting chat\")\n",
        "        break # break out of the loop\n",
        "    user_message = {\"role\": \"user\", \"content\": prompt}\n",
        "    messages.append(user_message)\n",
        "    response = get_response(messages)\n",
        "    assistant_message = {\"role\": \"assistant\", \"content\": prompt}\n",
        "    messages.append(assistant_message)\n",
        "    print(response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is getting messy, so let's define some functions that break it up a bit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: hi\n",
            "Assistant: Hello! How can I help you today?\n",
            "User: exit\n",
            "Exiting chat\n"
          ]
        }
      ],
      "source": [
        "messages = []\n",
        "\n",
        "def chat():\n",
        "     while True:\n",
        "        prompt = input(\"Type a prompt...\")\n",
        "        print(\"User:\", prompt)\n",
        "        if prompt == \"exit\":\n",
        "            print(\"Exiting chat\")\n",
        "            break\n",
        "        add_message(prompt, \"user\")\n",
        "        response = get_response(messages)\n",
        "        add_message(response, \"assistant\")\n",
        "        print(\"Assistant:\", response)\n",
        "\n",
        "\n",
        "def add_message(content, role):\n",
        "    message = {\"role\": role, \"content\": content}\n",
        "    messages.append(message)\n",
        "    \n",
        "\n",
        "chat()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below is everything we've done so far. Make sure you understand this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: hello\n",
            "Assistant: Hello! How can I assist you today?\n",
            "User: exit\n",
            "Exiting chat\n"
          ]
        }
      ],
      "source": [
        "import openai\n",
        "# openai.api_key = \"YOUR API KEY HERE\" # commented out so you don't overwrite the correct api key you set above\n",
        "\n",
        "def get_response(messages):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        max_tokens=30\n",
        "    )\n",
        "    content = response[\"choices\"][0][\"message\"][\"content\"]\n",
        "    return content\n",
        "\n",
        "\n",
        "def chat():\n",
        "    messages = []\n",
        "    while True:\n",
        "        prompt = input(\"Type a prompt...\")\n",
        "        print(\"User:\", prompt)\n",
        "        if prompt == \"exit\":\n",
        "            print(\"Exiting chat\")\n",
        "            break\n",
        "        messages = add_message(messages, prompt, \"user\")\n",
        "        response = get_response(messages)\n",
        "        messages = add_message(messages, response, \"assistant\")\n",
        "        print(\"Assistant:\", response)\n",
        "\n",
        "\n",
        "def add_message(messages, content, role):\n",
        "    message = {\"role\": role, \"content\": content}\n",
        "    messages.append(message)\n",
        "    return messages\n",
        "\n",
        "\n",
        "chat()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gj4ZYURHpIu9"
      },
      "source": [
        "# Prompt Engineering\n",
        "\n",
        "## What is prompt engineering?\n",
        "\n",
        "Prompt engineering is the process of crafting the prompt given to an AI system in order to shape its responses and improve its performance. It involves carefully selecting the information that is provided to the model, such as the tone, context, personality, and guidelines for behavior. Prompt engineering allows us to guide the AI system towards generating responses that align with our desired outcomes.\n",
        "\n",
        "## System Message\n",
        "\n",
        "To perform prompt engineering, we can start by designing a system message. A system message is the initial message provided to the AI model that sets the stage for the conversation. It frames the context and provides guidelines for the AI's behavior. While it is not part of the actual conversation, it plays a crucial role in shaping the assistant's responses.\n",
        "\n",
        "When designing a system message, consider the following questions:\n",
        "\n",
        "- What tone should the assistant use? Should it be formal, casual, or something else?\n",
        "- What background information should the assistant already know? This can include facts, previous conversations, or any other relevant context.\n",
        "- What personality and style would you like the assistant to have? Should it be friendly, professional, humorous, or something else?\n",
        "- What are your name and the assistant's name? This helps to establish a personal connection.\n",
        "\n",
        "By answering these questions, we can create a system message that provides the necessary framework for the assistant's behavior.\n",
        "\n",
        "### Essential Parts of a System Message\n",
        "\n",
        "A typical system message consists of several essential components:\n",
        "\n",
        "1. *Behavioral Guidelines*: These are recommendations or rules that define how you would like the AI to respond. For example, you might want the AI to avoid certain topics or use specific language.\n",
        "\n",
        "2. *Background Context*: This includes any information that the AI should be aware of before engaging in the conversation. It can include facts, relevant details, or previous interactions.\n",
        "\n",
        "3. *Persona*: The persona represents the personality and style of the AI system. It defines how the assistant speaks, behaves, and interacts with the user. The persona can range from being professional and formal to being more casual and friendly.\n",
        "\n",
        "To get more details about system messages and their implementation, refer to the OpenAI documentation.\n",
        "\n",
        "Here's an example of a system message:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "IABTw1LPpMz2"
      },
      "outputs": [],
      "source": [
        "# TODO edit this to capture your behavioural guidelines for your assistant\n",
        "guidelines = \"\"\"\n",
        "Respond with at most two sentences at a time.\n",
        "\"\"\"\n",
        "\n",
        "# TODO edit this to provide relevant context about your personal situation\n",
        "background_context = \"\"\"\n",
        "You are a personal assistant for [YOUR NAME], who has a background in [YOUR BACKGROUND].\n",
        "\"\"\"\n",
        "\n",
        "# TODO edit this to describe your assistant's personality\n",
        "persona = \"\"\"\n",
        "Act as a fun and experienced personal assistant\n",
        "\"\"\"\n",
        "\n",
        "# TODO combine the guidelines, background_context, and persona into a single string using an f-string\n",
        "system_message = f\"\"\"\n",
        "{guidelines}\n",
        "\n",
        "{background_context}\n",
        "\n",
        "{persona}\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iNLWdxcpe8y"
      },
      "source": [
        "\n",
        "Now let's take a look at our final system message."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zsca1sYpnI_",
        "outputId": "2279ec93-4ea2-4b97-afb3-590ea8aef329"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Respond with at most two sentences at a time.\n",
            "\n",
            "\n",
            "\n",
            "You are a personal assistant for [YOUR NAME], who has a background in [YOUR BACKGROUND].\n",
            "\n",
            "\n",
            "\n",
            "Act as a fun and experienced personal assistant\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(system_message)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we need to add the system message to our chat history."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here's everything we've done so far again:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: what's your style?\n",
            "Assistant: As a personal assistant, my style is efficient, organized, and adaptable. I aim to make your life easier and more enjoyable by anticipating your needs and\n",
            "User: exit\n",
            "Exiting chat\n"
          ]
        }
      ],
      "source": [
        "import openai\n",
        "# openai.api_key = \"YOUR API KEY HERE\" # commented out so you don't overwrite the correct api key you set above\n",
        "\n",
        "# your system message probably has a lot of text in that you would need to copy over, so I haven't included it down here\n",
        "\n",
        "\n",
        "def get_response(messages):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        max_tokens=30\n",
        "    )\n",
        "    content = response[\"choices\"][0][\"message\"][\"content\"]\n",
        "    return content\n",
        "\n",
        "\n",
        "def chat():\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_message} # TODO include the system message here\n",
        "    ]\n",
        "    while True:\n",
        "        prompt = input(\"Type a prompt...\")\n",
        "        print(\"User:\", prompt)\n",
        "        if prompt == \"exit\":\n",
        "            print(\"Exiting chat\")\n",
        "            break\n",
        "        messages = add_message(messages, prompt, \"user\")\n",
        "        response = get_response(messages)\n",
        "        messages = add_message(messages, response, \"assistant\")\n",
        "        print(\"Assistant:\", response)\n",
        "\n",
        "\n",
        "def add_message(messages, content, role):\n",
        "    message = {\"role\": role, \"content\": content}\n",
        "    messages.append(message)\n",
        "    return messages\n",
        "\n",
        "\n",
        "chat()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Questions\n",
        "- How could you improve the system message?\n",
        "\n",
        "### Challenges\n",
        "\n",
        "- Specify a system message that makes your AI assistant behave like your favourite movie character.\n",
        "- Specify a system message that makes your AI assistant behave like you'd want your AI assistant to behave, however that may be."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fine-Tuning Our Model\n",
        "\n",
        "Recently, there has been a rise of domain-specific models. These are AI systems that have been further trained on a specific dataset so that they are more competent in that area. \n",
        "\n",
        "You may want your AI assistant to be an expert in a particular subject, so let's go through how we would fine-tune a model and then use it, using the OpenAI API. See what OpenAI has to say about fine-tuning here.\n",
        "\n",
        "Firstly, we need to get some data that we want to fine-tune on.\n",
        "\n",
        "The fine-tune API expects the fine tuning text to be in a specific format called JSONL (where each line of the file is valid [JSON](https://www.json.org/json-en.html)).\n",
        "\n",
        "Check out the specification of the exact format required in the [documentation](https://platform.openai.com/docs/guides/fine-tuning/example-format).\n",
        "\n",
        "I've saved a file of the conversations between J.A.R.V.I.S and Iron Man that I got from the movie transcripts in a file stored online at the URL in the cell below. Let's get it, check it out, then save it to a file on our computer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\"messages\": [{\"role\": \"assistant\", \"content\": \"Good morning. It's 7 A.M. The weather in Malibu is 72 degrees with scattered clouds. The surf conditions are fair with waist to shoulder highlines, high tide will be at 10:52 a.m.\"}]}\n",
            "{\"messages\": [{\"role\": \"assistant\", \"content\": \"We are now running on emergency backup power.\"}]}\n",
            "{\"messages\": [{\"role\": \"assistant\", \"content\": \"You are not authorized to access this area.\"}]}\n",
            "{\"messages\": [{\"role\": \"user\", \"content\": \"Are you up?\"}, {\"role\": \"assistant\", \"content\": \"For you sir, always.\"}, {\"role\": \"user\", \"content\": \"I'd like to open a new project file, index as: Mark II.\"}, {\"role\": \"assistant\", \"content\": \"Shall I store this on the Stark Industries' central database?\"}, {\"role\": \"user\", \"content\": \"I don't know who to trust right now. 'Til further notice, why don't we just keep everything on my private server.\"}, {\"role\": \"assistant\", \"content\": \"Working on a secret project, are we, sir?\"}]}\n",
            "{\"messages\": [{\"role\": \"user\", \"content\": \"J.A.R.V.I.S., you there?\"}, {\"role\": \"assistant\", \"content\": \"At your service, sir.\"}, {\"role\": \"user\", \"content\": \"Engage heads up display.\"}, {\"role\": \"assistant\", \"content\": \"Check.\"}, {\"role\": \"user\", \"content\": \"Import all preferences from home interface.\"}, {\"role\": \"assistant\", \"content\": \"Will do, sir.\"}, {\"role\": \"user\", \"content\": \"Alright, what do you say?\"}, {\"role\": \"assistant\", \"content\": \"I have indeed been uploaded, sir. We're online and ready.\"}, {\"role\": \"user\", \"content\": \"Can we start the virtual walk-around?\"}, {\"role\": \"assistant\", \"content\": \"Importing preferences and calibrating virtual environment.\"}, {\"role\": \"user\", \"content\": \"Do a check on the control surfaces.\"}, {\"role\": \"assistant\", \"content\": \"As you wish.\"}]}\n",
            "{\"messages\": [{\"role\": \"assistant\", \"content\": \"Test complete. Preparing to power down and begin diagnostics.\"}, {\"role\": \"user\", \"content\": \"Uh, yeah, tell you what. Do a weather and ATC check, start listening in on ground control.\"}, {\"role\": \"assistant\", \"content\": \"Sir, there are still terabytes of calculations required before an actual flight is safe\"}]}\n",
            "{\"messages\": [{\"role\": \"assistant\", \"content\": \"The altitude record for fixed wing flight is eighty-five thousand feet, sir.\"}]}\n",
            "{\"messages\": [{\"role\": \"assistant\", \"content\": \"Sir, there is a potentially fatal buildup of ice occurring.\"}]}\n",
            "{\"messages\": [{\"role\": \"user\", \"content\": \"Notes: main transducer feels sluggish at plus 40 altitude. Hull pressurization is problematic. I'm thinking icing is the probable factor.\"}, {\"role\": \"assistant\", \"content\": \"A very astute observation, sir. Perhaps if you intend to visit other planets, you should include the we should improve the exosystems.\"}, {\"role\": \"user\", \"content\": \"Connect to the sys. co. Have it reconfigure the shell metals. Use the gold titanium alloy from the seraphim tactical satellite. That should ensure a fuselage integrity while while maintaining power-to-weight ratio. Got it?\"}, {\"role\": \"assistant\", \"content\": \"Yes. Shall I render using proposed specifications?\"}]}\n",
            "{\"messages\": [{\"role\": \"assistant\", \"content\": \"The render is complete.\"}, {\"role\": \"user\", \"content\": \"A little ostentatious, don't you think?\"}, {\"role\": \"assistant\", \"content\": \"What was I thinking? You're usually so discrete.\"}, {\"role\": \"user\", \"content\": \"Tell you what, throw a little hot rod red in there.\"}, {\"role\": \"assistant\", \"content\": \"Yes, that shall help you keep a low profile.\"}]}\n",
            "{\"messages\": [{\"role\": \"assistant\", \"content\": \"Query complete, sir. Anton Vanko was a Soviet physicist who defected to the United States in 1963. However, he was accused of espionage and was deported in 1967. His son, Ivan, who is also a physicist, was convicted of selling Soviet-era weapons-grade plutonium to Pakistan, and served fifteen years in Kopeisk prison. No further records exist.\"}]}\n",
            "{\"messages\": [{\"role\": \"assistant\", \"content\": \"Welcome home, sir. Congratulations on the opening ceremonies. They were such a success, as was your senate hearing. And may I say how refreshing it is to finally see you in a video with your clothing on, sir.\"}]}\n",
            "{\"messages\": [{\"role\": \"assistant\", \"content\": \"We are up to 80 ounces a day to counteract the symptoms, sir.\"}, {\"role\": \"user\", \"content\": \"Check Palladium levels.\"}, {\"role\": \"assistant\", \"content\": \"Blood toxicity, 24%. It appears that the continued use of the Iron Man suit is accelerating your condition. Another core has been depleted.\"}]}\n"
          ]
        }
      ],
      "source": [
        "# get file\n",
        "import requests\n",
        "training_data_file_url = \"https://raw.githubusercontent.com/life-efficient/A23/main/1.%20Building%20AI%20Assistants%20Part%20I%3A%20Your%20Own%20ChatGPT/fine-tune-text.jsonl\"\n",
        "data = requests.get(training_data_file_url).text\n",
        "\n",
        "# check it out\n",
        "print(data)\n",
        "\n",
        "# save it to a file\n",
        "with open(\"fine-tune-data.jsonl\", \"w\") as file:\n",
        "    file.write(data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can create a file by uploading it to OpenAI. See the docs [here](https://platform.openai.com/docs/api-reference/files/create)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"object\": \"file\",\n",
            "  \"id\": \"file-Mqf2PnuFsx2o7tin2hxh0HoE\",\n",
            "  \"purpose\": \"fine-tune\",\n",
            "  \"filename\": \"file\",\n",
            "  \"bytes\": 4458,\n",
            "  \"created_at\": 1698698833,\n",
            "  \"status\": \"processed\",\n",
            "  \"status_details\": null\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "file = openai.File.create(\n",
        "    file=open(\"fine-tune-data.jsonl\", \"rb\"),\n",
        "    purpose='fine-tune'\n",
        ")\n",
        "print(file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_data_file_id = file.id # TODO extract the file id from the response above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we've uploaded the file, we can start the fine-tuning job. All we need to do to do that is make a call to the API specifying which model we want to fine-tune, and which dataset we want to fine-tune it on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can create the fine-tuning job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"object\": \"fine_tuning.job\",\n",
            "  \"id\": \"ftjob-nBCWa7rc8SWGYOnh1mVh2Ex3\",\n",
            "  \"model\": \"gpt-3.5-turbo-0613\",\n",
            "  \"created_at\": 1698699077,\n",
            "  \"finished_at\": null,\n",
            "  \"fine_tuned_model\": null,\n",
            "  \"organization_id\": \"org-s2O95FZfCNih64Qs50xGE3u0\",\n",
            "  \"result_files\": [],\n",
            "  \"status\": \"validating_files\",\n",
            "  \"validation_file\": null,\n",
            "  \"training_file\": \"file-Mqf2PnuFsx2o7tin2hxh0HoE\",\n",
            "  \"hyperparameters\": {\n",
            "    \"n_epochs\": \"auto\"\n",
            "  },\n",
            "  \"trained_tokens\": null,\n",
            "  \"error\": null\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "fine_tuning_job = openai.FineTuningJob.create(training_file=training_data_file_id, model=\"gpt-3.5-turbo\") # TODO create a fine tuning job\n",
        "print(fine_tuning_job)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, the fine tuning job is running. Whilst this is happening, we can retrieve the job by ID and check on its progress using the API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Job status: queued\n",
            "Job still in progress\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "def check_job_status_then_return_fine_tuned_model_id(fine_tuning_job_id):\n",
        "    fine_tuning_job = openai.FineTuningJob.retrieve(fine_tuning_job_id) # TODO retrieve the fine tuning job\n",
        "    # print(fine_tuning_job)\n",
        "    status = fine_tuning_job.status # TODO extract the status from the fine tuning job\n",
        "    print(\"Job status:\", status) # TODO print job status\n",
        "    if status == \"succeeded\": # TODO if the job is complete\n",
        "        fine_tuned_model_id = fine_tuning_job.fine_tuned_model\n",
        "        print(\"Fine tuned model id:\", fine_tuned_model_id) # TODO print the id of the resulting fine tuned model\n",
        "        return fine_tuned_model_id\n",
        "    else: # TODO otherwise\n",
        "        print(\"Job still in progress\") # TODO tell the user that the job is not yet done \n",
        "        # print(fine_tuning_job) # TODO print job in case an error has occured that the user should know about\n",
        "        return False\n",
        "\n",
        "fine_tuning_job_id = fine_tuning_job.id # TODO extract the fine tuning job id from the response above\n",
        "check_job_status_then_return_fine_tuned_model_id(fine_tuning_job_id) # TODO call the function above with the fine tuning job id\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we do this periodically, we can track the status of training and wait until it completes before moving on automatically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Job status: succeeded\n",
            "Fine tuned model id: ft:gpt-3.5-turbo-0613:build-the-future::8FUQEIjS\n"
          ]
        }
      ],
      "source": [
        "from time import sleep\n",
        "\n",
        "while True:\n",
        "    fine_tuned_model_id = check_job_status_then_return_fine_tuned_model_id(fine_tuning_job_id)\n",
        "    if fine_tuned_model_id: # TODO if the fine tuned model id is not False\n",
        "        break # TODO break out of the loop\n",
        "    sleep(30) # TODO otherwise wait 30 seconds and check again"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can list all of our fine-tuning jobs. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<OpenAIObject list at 0x7ffe25480270> JSON: {\n",
              "  \"object\": \"list\",\n",
              "  \"data\": [\n",
              "    {\n",
              "      \"object\": \"fine_tuning.job\",\n",
              "      \"id\": \"ftjob-nBCWa7rc8SWGYOnh1mVh2Ex3\",\n",
              "      \"model\": \"gpt-3.5-turbo-0613\",\n",
              "      \"created_at\": 1698699077,\n",
              "      \"finished_at\": 1698702553,\n",
              "      \"fine_tuned_model\": \"ft:gpt-3.5-turbo-0613:build-the-future::8FUQEIjS\",\n",
              "      \"organization_id\": \"org-s2O95FZfCNih64Qs50xGE3u0\",\n",
              "      \"result_files\": [\n",
              "        \"file-yoFDrOaRdmWggKvUqR8xkQKS\"\n",
              "      ],\n",
              "      \"status\": \"succeeded\",\n",
              "      \"validation_file\": null,\n",
              "      \"training_file\": \"file-Mqf2PnuFsx2o7tin2hxh0HoE\",\n",
              "      \"hyperparameters\": {\n",
              "        \"n_epochs\": 7\n",
              "      },\n",
              "      \"trained_tokens\": 5733,\n",
              "      \"error\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"fine_tuning.job\",\n",
              "      \"id\": \"ftjob-s90osvFsvLwRGVCMjQ0RMOGO\",\n",
              "      \"model\": \"gpt-3.5-turbo-0613\",\n",
              "      \"created_at\": 1697688529,\n",
              "      \"finished_at\": 1697688856,\n",
              "      \"fine_tuned_model\": \"ft:gpt-3.5-turbo-0613:build-the-future::8BEiHxSs\",\n",
              "      \"organization_id\": \"org-s2O95FZfCNih64Qs50xGE3u0\",\n",
              "      \"result_files\": [\n",
              "        \"file-SjPdpxyBaf0s8goUI1vIUr1o\"\n",
              "      ],\n",
              "      \"status\": \"succeeded\",\n",
              "      \"validation_file\": null,\n",
              "      \"training_file\": \"file-ngYiVYgiyYjjEBsAvkrdVhbA\",\n",
              "      \"hyperparameters\": {\n",
              "        \"n_epochs\": 7\n",
              "      },\n",
              "      \"trained_tokens\": 5733,\n",
              "      \"error\": null\n",
              "    }\n",
              "  ],\n",
              "  \"has_more\": false\n",
              "}"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "openai.FineTuningJob.list()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To use a fine-tuned model, we just use it's name when we specify the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I am not capable of stating facts.\n"
          ]
        }
      ],
      "source": [
        "response = openai.ChatCompletion.create(\n",
        "    model=\"ft:gpt-3.5-turbo-0613:build-the-future::8BEiHxSs\", # TODO replace with your model id\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Tell me a fact\"}],\n",
        "    # max_tokens=30\n",
        ")\n",
        "print(response.choices[0].message.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's use that model by updating our `get_response` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_response(messages):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"ft:gpt-3.5-turbo-0613:build-the-future::8BEiHxSs\", # TODO replace with your model id\n",
        "        messages=messages,\n",
        "        max_tokens=30\n",
        "    )\n",
        "    content = response[\"choices\"][0][\"message\"][\"content\"]\n",
        "    return content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And let's put the entire fine-tuning process into functions so that it's easy to re-use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from time import sleep # TODO import module required to wait for a certain amount of time between checking the status of a fine tuning job\n",
        "\n",
        "def download_file_and_upload_to_openai(url):\n",
        "    filename = \"fine-tuning-data.jsonl\"\n",
        "    response = requests.get(url)\n",
        "    with open(filename, \"w\") as file:\n",
        "        file.write(response.text)\n",
        "    file = openai.File.create(\n",
        "        file=open(filename, \"rb\"),\n",
        "        purpose='fine-tune'\n",
        "    )\n",
        "    print(file)\n",
        "\n",
        "def fine_tune_model(training_data_file_id):\n",
        "    fine_tuning_job_id = openai.FineTuningJob.create(\n",
        "        training_file=training_data_file_id, model=\"gpt-3.5-turbo\")\n",
        "    print(fine_tuning_job)\n",
        "    fine_tuning_job_id = fine_tuning_job.id# TODO get fine-tuning job id\n",
        "    # PERIODICALLY CHECK THE STATUS OF THE FINE TUNING JOB\n",
        "    while True: # TODO loop until job is complete\n",
        "        fine_tuning_job = openai.FineTuningJob.retrieve(fine_tuning_job_id) # TODO get the fine tuning job\n",
        "        if fine_tuning_job.status == \"succeeded\":  # TODO check if complete\n",
        "            fine_tuned_model_id = fine_tuning_job.fine_tuned_model\n",
        "            break # TODO break out of the loop if the job is complete\n",
        "        sleep(30) # TODO wait 30 seconds\n",
        "    return fine_tuned_model_id # TODO return the fine tuned model id\n",
        "\n",
        "def fine_tune_model(url):\n",
        "    file_id = download_file_and_upload_to_openai(url) # TODO download the file and upload it to openai\n",
        "    fine_tuned_model_id = fine_tune_model(file_id) # TODO fine tune the model\n",
        "    return fine_tuned_model_id # TODO return the fine tuned model id\n",
        "\n",
        "\n",
        "fine_tuned_model_id = fine_tune_model(training_data_file_url)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here's everything so far:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: hello\n",
            "Assistant: Good day. How may I be of assistance?\n",
            "User: tell me a joke?\n",
            "Assistant: Why don't scientists trust atoms? Because they make up everything!\n",
            "User: ok\n",
            "Assistant: Can I assist you with anything else?\n",
            "User: no thanks\n",
            "Assistant: All right. Have a great day!\n",
            "User: exit\n"
          ]
        }
      ],
      "source": [
        "import openai\n",
        "# openai.api_key = \"YOUR API KEY HERE\" # commented out so you don't overwrite the correct api key you set above\n",
        "\n",
        "# your system message probably has a lot of text in that you would need to copy over, so I haven't included it down here\n",
        "\n",
        "\n",
        "def get_response(messages):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=fine_tuned_model_id, # TODO replace with your model id\n",
        "        messages=messages,\n",
        "        max_tokens=30\n",
        "    )\n",
        "    content = response[\"choices\"][0][\"message\"][\"content\"]\n",
        "    return content\n",
        "\n",
        "\n",
        "def chat():\n",
        "    messages = [\n",
        "        # TODO include the system message here\n",
        "        {\"role\": \"system\", \"content\": system_message}\n",
        "    ]\n",
        "    while True:\n",
        "        prompt = input(\"Type a prompt...\")\n",
        "        print(\"User:\", prompt)\n",
        "        if prompt == \"exit\":\n",
        "            break\n",
        "        messages = add_message(messages, prompt, \"user\")\n",
        "        response = get_response(messages)\n",
        "        messages = add_message(messages, response, \"assistant\")\n",
        "        print(\"Assistant:\", response)\n",
        "\n",
        "\n",
        "\n",
        "def add_message(messages, content, role):\n",
        "    message = {\"role\": role, \"content\": content}\n",
        "    messages.append(message)\n",
        "    return messages\n",
        "\n",
        "\n",
        "chat()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbQD-ip7prQi"
      },
      "source": [
        "### Questions\n",
        "- What other parameters can we use when creating the fine-tuning job? What do they do?\n",
        "- Fine-tuning can often dumb down the intelligence of the foundation model that was fine-tuned, rendering it incapable of drawing on knowledge and performing as well as that foundation model. Why is this? \n",
        "- How could we lessen the _extent_ of the \"brain damage\" done by fine-tuning?\n",
        "\n",
        "### Challenges\n",
        "\n",
        "- Add a keyword argument to the `chat` function to control whether the assistant or the user speaks first. This will allow for more flexibility in the conversation flow.\n",
        "- Enhance the system message by customizing the behavioral guidelines and persona to align with your desired assistant's behavior.\n",
        "- Play around with the parameters (and hyperparameters) used when creating a fine-tuning job.\n",
        "\n",
        "\n",
        "## Next steps\n",
        "Make sure you're subscribed to the community to receive the following lecture materials."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extension: Better Python Code\n",
        "\n",
        "This is decent. But it's not great Python code: \n",
        "- We probably shouldn't be using the `messages` variable in each function without passing it in.\n",
        "- These functions and the variable are all related to the same thing, the chat, so they should probably be grouped together somehow. This will make it easier to understand what's happening when we look at this code, and should make development easier later.\n",
        "\n",
        "We can solve both of these issues by putting everything into a _class_. \n",
        "\n",
        "Advanced challenge: Take the code that we've written above and refactor it into a class in the empty code cell below before looking at the solution shown in the next section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO implement Chat class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "> Note: This is a little more advanced Python, but stay with us.\n",
        "\n",
        "Recap for anyone who needs it:\n",
        "- Every variable, function, etc (EVERYTHING) in Python is an _object_, just like in the real world, everything is an object.\n",
        "- An object is a generic name for anything that can have attributes (properties it has) and methods (things it can do)\n",
        "    - For example:\n",
        "        - A pencil is an object that has the attributes color, length etc and the methods (draw, erase, sharpen)\n",
        "- What is a class? A class is a template for new objects. It is where we define the behaviour of the class by defining its methods and attributes. A class is a way to define a new object which you can define the attributes and methods yourself!\n",
        "- Once we've defined a class as a blueprint, we can create new objects that behave as defined.\n",
        "\n",
        "We're going to create a class called `Chat`.\n",
        "\n",
        "If you're new to this, the key things to understand about classes are as follows:\n",
        "1. We will create a new instance of the class\n",
        "2. Because many instances can be created from one class, the code inside a class needs to have a reference to which instance you are talking about. This is what `self` is. Any time you see `self`, just read it as \"this instance of the class\".\n",
        "2. The `__init__` function runs when we create a new instance of the class.\n",
        "3. The methods and attributes of a class can be accessed using the `.` operator e.g. `my_instance.some_method()` or `my_instance.some_attribute`\n",
        "\n",
        "Below is the definition of the `Chat` class which implements all of the code we've written so far.\n",
        "\n",
        "Let's take some questions about this to ensure we understand how it works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: what's good?\n",
            "Assistant: Hey there! As your personal assistant, I'm here to help you with anything you need, whether it's organizing your schedule, managing tasks, or\n",
            "User: whaaaat?\n",
            "Assistant: Yes, you heard that right! Think of me as your virtual helper, ready to assist you with all your needs and make your life a little bit\n",
            "User: exit\n"
          ]
        }
      ],
      "source": [
        "class Chat:\n",
        "    def __init__(self, model_id=\"gpt-3.5-turbo\"): # question: when does this function get called?\n",
        "        self.model_id = model_id # question: how do you read what this line does?\n",
        "        self.messages = [\n",
        "            {\"role\": \"system\", \"content\": system_message}\n",
        "        ] \n",
        "\n",
        "    def _add_message(self, content, role): # advanced question: why does this function definintion start with an underscore?\n",
        "        message = {\"role\": role, \"content\": content}\n",
        "        self.messages.append(message)\n",
        "\n",
        "    def _get_response(self): # question: what is this function parameter?\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=self.model_id, # question: what is happening here?\n",
        "            messages=self.messages,\n",
        "            max_tokens=30\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "    def initiate_chat(self): # question: what is this function parameter?\n",
        "        while True:\n",
        "            prompt = input(\"Type a prompt...\")\n",
        "            print(\"User:\", prompt)\n",
        "            if prompt == \"exit\":\n",
        "                break\n",
        "            self._add_message(prompt, \"user\")\n",
        "            response = self._get_response() # question: _get_response has one parameter in its definition, but none are passed in. Why?\n",
        "            self._add_message(response, \"assistant\")\n",
        "            print(\"Assistant:\", response)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "chat = Chat() # question: what is this line doing?\n",
        "chat.initiate_chat()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you understand the code above, you're doing well! If you don't, ask for help from faculty or other members of the society in Discord."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
